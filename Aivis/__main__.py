#!/usr/bin/env python3

# FutureWarning / RuntimeWarning / UserWarning ã‚’æŠ‘åˆ¶ã™ã‚‹
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
warnings.simplefilter(action='ignore', category=RuntimeWarning)
warnings.simplefilter(action='ignore', category=UserWarning)

import json
import re
import shutil
import subprocess
import sys
import typer
from pathlib import Path
from typing import Annotated, Any, cast, Optional

from Aivis import __version__
from Aivis import constants
from Aivis import demucs
from Aivis import prepare
from Aivis import utils


app = typer.Typer(help='Aivis: AI Voice Imitation System')

@app.command(help='Create audio segments from audio sources.')
def create_segments(
    whisper_model: Annotated[constants.ModelNameType, typer.Option(help='Whisper model name.')] = constants.ModelNameType.large_v3,
    force_transcribe: Annotated[bool, typer.Option(help='Force Whisper to transcribe audio files.')] = False,
):
    # ã“ã®ã‚µãƒ–ã‚³ãƒãƒ³ãƒ‰ã§ã—ã‹åˆ©ç”¨ã›ãšã€ã‹ã¤æ¯”è¼ƒçš„ã‚¤ãƒ³ãƒãƒ¼ãƒˆãŒé‡ã„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã“ã“ã§ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹
    import faster_whisper
    import stable_whisper

    # 01-Sources ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä»¥ä¸‹ã®ãƒ¡ãƒ‡ã‚£ã‚¢ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    ## æ‹¡å¼µå­ã¯ .wav / .mp3 / .m4a / .mp4 / .ts
    ## ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆé †ã«ã‚½ãƒ¼ãƒˆã™ã‚‹
    source_files = sorted(list(constants.SOURCES_DIR.glob('**/*.*')))
    source_files = [i for i in source_files if i.suffix in constants.SOURCE_FILE_EXTENSIONS]

    # Demucs V4 (htdemucs_ft) ã§ AI éŸ³æºåˆ†é›¢ã‚’è¡Œã„ã€éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒœã‚¤ã‚¹ã®ã¿ã‚’æŠ½å‡ºã™ã‚‹
    ## æœ¬æ¥ã¯æ¥½æ›²ã‚’ãƒœãƒ¼ã‚«ãƒ«ãƒ»ãƒ‰ãƒ©ãƒ ãƒ»ãƒ™ãƒ¼ã‚¹ãƒ»ãã®ä»–ã«éŸ³æºåˆ†é›¢ã™ã‚‹ãŸã‚ã® AI ã ãŒã€ã“ã‚Œã‚’å¿œç”¨ã—ã¦ BGMãƒ»SEãƒ»ãƒã‚¤ã‚ºãªã©ã‚’å¤§éƒ¨åˆ†é™¤å»ã§ãã‚‹
    ## Demucs ã§ãƒœãƒ¼ã‚«ãƒ« (=ãƒœã‚¤ã‚¹) ã®ã¿ã‚’æŠ½å‡ºã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã¯ 02-PreparedSources/(éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«å).wav ã«å‡ºåŠ›ã•ã‚Œã‚‹
    ## ã™ã§ã«æŠ½å‡ºæ¸ˆã¿ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆã¯éŸ³æºåˆ†é›¢ã¯è¡Œã‚ã‚Œãšã€ã™ã§ã«æŠ½å‡ºæ¸ˆã¿ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹
    voices_files = demucs.ExtractVoices(source_files, constants.PREPARE_SOURCES_DIR)

    model: faster_whisper.WhisperModel | None = None

    # ã“ã“ã‹ã‚‰ã¯å„éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«ãƒ«ãƒ¼ãƒ—
    for voices_file in voices_files:
        typer.echo('=' * utils.GetTerminalColumnSize())

        # å‡ºåŠ›å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        ## ã™ã§ã«å­˜åœ¨ã—ã¦ã„ã‚‹å ´åˆã¯ç”Ÿæˆæ¸ˆã¿ãªã®ã§ã‚¹ã‚­ãƒƒãƒ— (ãŸã ã—ã€ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä¸­èº«ãŒç©ºã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ã—ãªã„)
        ## ã‚‚ã—ã‚‚ã†ä¸€åº¦ç”Ÿæˆã—ãŸã„å ´åˆã¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤ã™ã‚‹ã“ã¨
        folder = constants.SEGMENTS_DIR / voices_file.name.split('.')[0]
        if folder.exists() and len(list(folder.glob('*.*'))) > 0:
            typer.echo(f'Directory {folder} already exists. Skip.')
            continue
        folder.mkdir(parents=True, exist_ok=True)
        typer.echo(f'Directory {folder} created.')

        transcribe_result: stable_whisper.WhisperResult
        results_json_file = constants.PREPARE_SOURCES_DIR / f'{voices_file.name.split(".")[0]}.json'

        # ã™ã§ã«éŸ³å£°èªè­˜çµæœã®ãƒ‡ãƒ¼ã‚¿ (JSON) ãŒä¿å­˜ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã„ã€æ–°è¦ã®éŸ³å£°èªè­˜ã¯è¡Œã‚ãªã„
        ## ãªãŠã€--force-transcribe ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ JSON ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ã«é–¢ã‚ã‚‰ãšéŸ³å£°èªè­˜ã‚’å®Ÿè¡Œã™ã‚‹
        if results_json_file.exists() and force_transcribe is False:
            typer.echo(f'File {voices_file} already transcribed.')
            transcribe_result = stable_whisper.WhisperResult(str(results_json_file))

        # Whisper ã§éŸ³å£°èªè­˜ã‚’å®Ÿè¡Œ
        else:

            typer.echo('-' * utils.GetTerminalColumnSize())
            typer.echo(f'File {voices_file} transcribing...')
            typer.echo('-' * utils.GetTerminalColumnSize())

            # Whisper ã®å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ (1å›ã®ã¿)
            if model is None:
                typer.echo(f'Whisper model loading... (Model: {whisper_model.value})')
                model = stable_whisper.load_faster_whisper(
                    whisper_model.value,
                    device = 'cuda',
                    compute_type = 'auto',
                )
                typer.echo('Whisper model loaded.')
                typer.echo('-' * utils.GetTerminalColumnSize())

            # Whisper ã«å…¥åŠ›ã™ã‚‹åˆæœŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ (å‘ªæ–‡)
            ## Whisper ã¯å‰ã®æ–‡è„ˆã‚’è¸ã¾ãˆã¦æ›¸ãèµ·ã“ã—ã¦ãã‚Œã‚‹ã‚‰ã—ã„ã®ã§ã€ä¼šè©±æ–‡ã®æ›¸ãèµ·ã“ã—ã£ã½ã„ã‚‚ã®ã‚’å…¥ã‚Œã¦ãŠãã¨ã€
            ## æ›¸ãèµ·ã“ã—ã«å¥èª­ç‚¹ã‚’ã¤ã‘ã‚‹ã‚ˆã†èª˜å°ã§ãã‚‹ã¿ãŸã„â€¦
            initial_prompt = (
                'ãã†ã ã€‚ä»Šæ—¥ã¯ãƒ”ã‚¯ãƒ‹ãƒƒã‚¯ã—ãªã„â€¦ï¼Ÿå¤©æ°—ã‚‚ã„ã„ã—ã€çµ¶å¥½ã®ãƒ”ã‚¯ãƒ‹ãƒƒã‚¯æ—¥å’Œã ã¨æ€ã†ï¼ è‰¯ã„ã§ã™ã­ï¼è¡Œãã¾ã—ã‚‡ã†â€¦ï¼'
                'ã˜ã‚ƒã‚æ—©é€Ÿã€è·ç‰©ã®æº–å‚™ã—ã¦ãŠãã¾ã™ã­ã€‚ ãã†ã ã­ï¼ã©ã“ã«è¡Œãï¼Ÿ ãã†ã§ã™ã­â€¦ã€‚æ¡œã®è¦‹ãˆã‚‹å…¬åœ’ãªã‚“ã‹ã©ã†ã§ã—ã‚‡ã†â€¦ï¼Ÿ'
                'ãŠãƒ¼ï¼ä»Šã®æ™‚æœŸã¯æ¡œãŒç¶ºéº—ã ã—ã­ã€‚ã˜ã‚ƒã‚ãã‚Œã§æ±ºã¾ã‚Šã£ï¼ åˆ†ã‹ã‚Šã¾ã—ãŸã€‚èª¿ã¹ãŸã¨ã“ã‚ã€é›»è»Šã ã¨550å††æ›ã‹ã‚‹ã¿ãŸã„ã§ã™ã€‚'
                'å°‘ã—æ™‚é–“ãŒæ›ã‹ã‚Šã¾ã™ãŒã€æ­©ã„ãŸæ–¹ãŒå¥åº·çš„ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚ ãˆã€œï¼æ­©ãã®ã¯ãã¤ã„ã‚ˆã‰â€¦ã€‚'
            )

            # éŸ³å£°èªè­˜ã‚’å®Ÿè¡Œã—ã€ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãªã©ãŒèª¿æ•´ã•ã‚ŒãŸéŸ³å£°èªè­˜çµæœã‚’å–å¾—ã™ã‚‹
            # ref: https://qiita.com/reriiasu/items/5ad8e1a7dbc425de7bb0
            # ref: https://zenn.dev/tsuzukia/articles/1381e6c9a88577
            # ref: https://note.com/asahi_ictrad/n/nf3ca329f17df
            transcribe_result: stable_whisper.WhisperResult = cast(Any, model).transcribe_stable(
                # å…¥åŠ›å…ƒã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«
                str(voices_file),
                # å˜èªã”ã¨ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’å‡ºåŠ›ã™ã‚‹
                word_timestamps = True,
                # ãƒ­ã‚°ã‚’ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«å‡ºåŠ›ã™ã‚‹
                verbose = True,
                # å˜èªã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®å†ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã‚’è¡Œã‚ãªã„
                ## åˆ¥é€”éŸ³å£°èªè­˜ãŒå®Œäº†ã—ã¦ã‹ã‚‰è¡Œã†
                regroup = False,
                # ã™ã§ã« Demucs ã§éŸ³æºåˆ†é›¢ã‚’è¡Œã£ã¦ã„ã‚‹ãŸã‚ã€ã“ã“ã§ã¯éŸ³æºåˆ†é›¢ã‚’è¡Œã‚ãªã„
                ## éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€ã‚ˆã‚Šã‚‚ã€èª­ã¿è¾¼ã‚“ã ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã„ã¾ã‚ã—ãŸæ–¹ãŒé«˜é€Ÿã«å‡¦ç†ã§ãã‚‹
                demucs = False,
                # Silero VAD ã‚’ä½¿ç”¨ã—ã¦ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æŠ‘åˆ¶ãƒã‚¹ã‚¯ã‚’ç”Ÿæˆã™ã‚‹
                vad = True,
                # faster-whisper æœ¬ä½“ã®è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                # æ—¥æœ¬èª
                language = 'ja',
                # beam_size (1 ã«è¨­å®šã—ã¦ CER ã‚’ä¸‹ã’ã‚‹)
                beam_size = 1,
                # è¬ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (10 ã«è¨­å®šã™ã‚‹ã¨ temperature ã‚’ä¸‹ã’ãŸã“ã¨ã§ä¸ŠãŒã‚‹ repetition ã‚’æŠ‘ãˆã‚‰ã‚Œã‚‹ã‚‰ã—ã„ï¼Ÿ)
                no_repeat_ngram_size = 10,
                # temperature (0.0 ã«è¨­å®šã—ã¦ CER ã‚’ä¸‹ã’ã‚‹)
                temperature = 0.0,
                # å‰å›ã®éŸ³å£°ãƒãƒ£ãƒ³ã‚¯ã®å‡ºåŠ›çµæœã‚’æ¬¡ã®ã‚¦ã‚¤ãƒ³ãƒ‰ã‚¦ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«è¨­å®šã—ãªã„
                condition_on_previous_text = False,
                # åˆæœŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
                initial_prompt = initial_prompt,
                # faster-whisper å´ã§ VAD ã‚’ä½¿ã£ãŸç„¡éŸ³ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’è¡Œã†
                vad_filter = True,
            )
            typer.echo('-' * utils.GetTerminalColumnSize())
            typer.echo(f'File {voices_file} transcribed.')

            # éŸ³å£°èªè­˜çµæœã‚’å†ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã™ã‚‹
            ## å†ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯å¤šãã‚ã‚‹ãŒã€ã“ã“ã§ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’èª¿æ•´ã—ã¦ä½¿ã£ã¦ã„ã‚‹
            ## ref: https://github.com/jianfch/stable-ts#regrouping-words
            (transcribe_result.clamp_max()
                .split_by_punctuation([('.', ' '), 'ã€‚', '?', 'ï¼Ÿ', (',', ' '), 'ï¼Œ'])  # type: ignore
                .split_by_gap(0.75)
                .merge_by_gap(0.3, max_words=3)
                .split_by_punctuation([('.', ' '), 'ã€‚', '?', 'ï¼Ÿ']))  # type: ignore

            # éŸ³å£°èªè­˜çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›ã™ã‚‹
            with open(results_json_file, mode='w', encoding='utf-8') as f:
                json.dump(transcribe_result.to_dict(), f, indent=4, ensure_ascii=False, allow_nan=True)

        # ä¸€æ–‡ã”ã¨ã«åˆ‡ã‚Šå‡ºã—ãŸéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã«ã¯æ›¸ãèµ·ã“ã—æ–‡ãŒå…¥ã‚‹ï¼‰ã‚’å‡ºåŠ›ã™ã‚‹
        count = 1
        for index, segment in enumerate(transcribe_result.segments):
            typer.echo('-' * utils.GetTerminalColumnSize())

            # æ›¸ãèµ·ã“ã—çµæœã‚’ä¸‹å‡¦ç†ã—ã€ã‚ˆã‚Šãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã—ã¦æœ€é©ãªå½¢ã«ã™ã‚‹
            transcript = prepare.PrepareText(segment.text)
            typer.echo(f'Transcript: {transcript}')

            # Whisper ã¯ç„¡éŸ³åŒºé–“ã¨ã‹ãŒã‚ã‚‹ã¨ã€Œè¦–è´é ‚ãã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸã€ã€Œãƒãƒ£ãƒ³ãƒãƒ«ç™»éŒ²ã‚ˆã‚ã—ãã€ãªã©ã®è¬ã®ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ãŒç™ºç”Ÿã™ã‚‹ã®ã§ã€
            # ãã†ã„ã†ç³»ã®æ›¸ãèµ·ã“ã—çµæœãŒã‚ã£ãŸå ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹
            if transcript in constants.SKIP_TRANSCRIPTS:
                typer.echo(f'Transcript skipped. (Transcript is in SKIP_TRANSCRIPTS)')
                continue

            # (å¥èª­ç‚¹å«ã‚ã¦) æ›¸ãèµ·ã“ã—çµæœãŒ4æ–‡å­—æœªæº€ã ã£ãŸå ´åˆã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã™ã‚‹ã«ã¯çŸ­ã™ãã‚‹ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹
            ## ä¾‹: ãã†ã€‚/ ã¾ã˜ï¼Ÿ / ã‚ã€‚
            if len(transcript) < 4:
                typer.echo(f'Transcript skipped. (Transcript length < 4 characters)')
                continue

            # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®é–‹å§‹æ™‚é–“ã¨çµ‚äº†æ™‚é–“ã‚’å–å¾—
            segment_start = segment.start
            segment_end = segment.end

            # ã‚‚ã—ç¾åœ¨å‡¦ç†ä¸­ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®æœ€åˆã®å˜èªã®é•·ã•ãŒ 0.425 ç§’ä»¥ä¸Šã ã£ãŸå ´åˆã€å…ˆé ­ 0.25 ç§’ã‚’å‰Šã‚‹
            ## å‰ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®æœ€å¾Œã®ç™ºéŸ³ã®æ¯éŸ³ãŒå«ã¾ã‚Œã¦ã—ã¾ã†å•é¡Œã®å›é¿ç­–
            ## æ—¥æœ¬èªã®å ´åˆå˜èªã¯åŸºæœ¬1æ–‡å­—ã‹2æ–‡å­—ã«ãªã‚‹ãŸã‚ã€ç™ºå£°æ™‚é–“ã¯ 0.425 ç§’ä»¥ä¸‹ã«ãªã‚‹ã“ã¨ãŒå¤šã„ã®ã‚’åˆ©ç”¨ã—ã¦ã„ã‚‹
            if segment.words[0].duration >= 0.425:
                segment_start += 0.25

                # ã•ã‚‰ã«ã€ã‚‚ã—ç¾åœ¨å‡¦ç†ä¸­ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®æœ€åˆã®å˜èªã®é•·ã•ãŒ 1 ç§’ä»¥ä¸Šã ã£ãŸå ´åˆã€
                # ãã®é•·ã• - 1 ç§’ã‚’ã•ã‚‰ã«å‰Šã‚‹ (æœ€ä½ã§ã‚‚ 0.75 ç§’ã¯æ®‹ã™)
                ## ä¾‹: 3.6 ç§’ã‚ã‚‹å˜èªãªã‚‰ã€å…ˆé ­ 0.25 ç§’ + 2.6 ç§’ = å…ˆé ­ 2.85 ç§’ã‚’å‰Šã‚Šã€æ®‹ã‚Šã® 0.75 ç§’ã‚’å‡ºåŠ›ã™ã‚‹
                ## 1å˜èªã®ç™ºå£°ã« 1 ç§’ä»¥ä¸Šæ›ã‹ã‚‹ã“ã¨ã¯ã»ã¼ã‚ã‚Šå¾—ãªã„ãŸã‚ã€ç„¡éŸ³åŒºé–“ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã¨åˆ¤æ–­ã™ã‚‹
                if segment.words[0].duration >= 1.0:
                    segment_start += segment.words[0].duration - 1.0

            # ã‚‚ã—æ¬¡ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®æœ€åˆã®å˜èªã®é•·ã•ãŒ 0.425 ç§’ä»¥ä¸Šã ã£ãŸå ´åˆã€æœ«å°¾ 0.25 ç§’ã‚’ä¼¸ã°ã™
            ## æœ€å¾Œã®ç™ºéŸ³ã®æ¯éŸ³ãŒåˆ‡ã‚Œã¦ã—ã¾ã†å•é¡Œã®å›é¿ç­–
            if index + 1 < len(transcribe_result.segments) and transcribe_result.segments[index + 1].words[0].duration >= 0.425:
                segment_end += 0.25

                # ã•ã‚‰ã«ã€ã‚‚ã—æ¬¡ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®æœ€åˆã®å˜èªã®é•·ã•ãŒ 1 ç§’ä»¥ä¸Šã ã£ãŸå ´åˆã€
                # ãã®é•·ã• - 1 ç§’ã‚’ã•ã‚‰ã«ä¼¸ã°ã™ (æœ€å¤§ã§ 1.0 ç§’ã¾ã§ä¼¸ã°ã™)
                if transcribe_result.segments[index + 1].words[0].duration >= 1.0:
                    segment_end += min(transcribe_result.segments[index + 1].words[0].duration - 1.0, 1.0)

            # ã‚‚ã—æ¬¡ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®é–‹å§‹ä½ç½®ãŒç¾åœ¨å‡¦ç†ä¸­ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®çµ‚äº†ä½ç½®ã‚ˆã‚Šã‚‚å¾Œãªã‚‰ã€
            # ç¾åœ¨å‡¦ç†ä¸­ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®çµ‚äº†ä½ç½®ã‚’æ¬¡ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®é–‹å§‹ä½ç½®ã«åˆã‚ã›ã¦æœ«å°¾ãŒæ¬ ã‘ãªã„ã‚ˆã†ã«ã™ã‚‹ (æœ€å¤§ã§ 3.0 ç§’ã¾ã§ä¼¸ã°ã™)
            if index + 1 < len(transcribe_result.segments) and segment_end < transcribe_result.segments[index + 1].start:
                segment_end = min(transcribe_result.segments[index + 1].start, segment_end + 3.0)

            # ã‚‚ã—ç¾åœ¨å‡¦ç†ä¸­ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãŒéŸ³å£°èªè­˜çµæœã®æœ€å¾Œã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãªã‚‰ã€
            # ç¾åœ¨å‡¦ç†ä¸­ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®çµ‚äº†ä½ç½®ã‚’éŸ³å£°ã®é•·ã•ã«åˆã‚ã›ã¦æœ«å°¾ãŒæ¬ ã‘ãªã„ã‚ˆã†ã«ã™ã‚‹
            if index + 1 == len(transcribe_result.segments):
                segment_end = prepare.GetAudioFileDuration(voices_file)

            typer.echo(f'Segment Range: {utils.SecondToTimeCode(segment_start)} - {utils.SecondToTimeCode(segment_end)}')

            # é–‹å§‹æ™‚åˆ»ã¨çµ‚äº†æ™‚åˆ»ãŒåŒã˜ã ã£ãŸå ´åˆã€ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãŒæ­£ã—ãå–å¾—ã§ãã¦ã„ãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹
            if segment_start == segment_end:
                typer.echo(f'Transcript skipped. (Start time == End time)')
                continue

             # å‡ºåŠ›ã™ã‚‹éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®é•·ã•ãŒ1ç§’æœªæº€ã«ãªã£ãŸå ´åˆã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã™ã‚‹ã«ã¯çŸ­ã™ãã‚‹ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹
            if segment_end - segment_start < 1:
                typer.echo(f'Transcript skipped. (Duration < 1 sec)')
                continue

            # å‡ºåŠ›å…ˆã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            # ä¾‹: 0001_ã“ã‚“ã«ã¡ã¯.wav
            output_audio_file = folder / f'{count:04d}_{transcript}.wav'

            # ä¸€æ–‡ã”ã¨ã«åˆ‡ã‚Šå‡ºã—ãŸ (ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåŒ–ã—ãŸ) éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡ºåŠ›
            real_output_audio_file = prepare.SliceAudioFile(voices_file, output_audio_file, segment_start, segment_end)

            typer.echo(f'File {real_output_audio_file} saved.')
            count += 1

    typer.echo('=' * utils.GetTerminalColumnSize())
    typer.echo('All files segmentation done.')
    typer.echo('=' * utils.GetTerminalColumnSize())


@app.command(help='Create datasets from audio segments.')
def create_datasets(
    segments_dir_name: Annotated[str, typer.Argument(help='Segments directory name. Glob pattern (wildcard) is available.')],
    speaker_names: Annotated[str, typer.Argument(help='Speaker name. (Comma separated)')],
):
    # ã“ã®ã‚µãƒ–ã‚³ãƒãƒ³ãƒ‰ã§ã—ã‹åˆ©ç”¨ã›ãšã€ã‹ã¤æ¯”è¼ƒçš„ã‚¤ãƒ³ãƒãƒ¼ãƒˆãŒé‡ã„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã“ã“ã§ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹
    import gradio

    typer.echo('=' * utils.GetTerminalColumnSize())

    # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    if speaker_names == '':
        typer.echo(f'Error: Speaker names is empty.')
        typer.echo('=' * utils.GetTerminalColumnSize())
        sys.exit(1)

    # å‡ºåŠ›å¾Œã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å‡ºåŠ›å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒãªã‘ã‚Œã°ä½œæˆ
    speaker_name_list = speaker_names.split(',')
    for speaker in speaker_name_list:
        output_dir = constants.DATASETS_DIR / speaker
        if not output_dir.exists():
            output_dir.mkdir(parents=True, exist_ok=True)
            typer.echo(f'Speaker: {speaker} / Directory: {output_dir} created.')
        else:
            typer.echo(f'Speaker: {speaker} / Directory: {output_dir} already created.')
    typer.echo('=' * utils.GetTerminalColumnSize())

    # 03-Segments/(æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã® Glob ãƒ‘ã‚¿ãƒ¼ãƒ³)/ ä»¥ä¸‹ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåŒ–ã•ã‚ŒãŸéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    ## æ‹¡å¼µå­ã¯ .wav
    ## glob() ã®çµæœã¯é †åºãŒãƒãƒ©ãƒãƒ©ãªã®ã§ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆé †ã«ã‚½ãƒ¼ãƒˆã™ã‚‹
    segment_audio_paths = sorted(list((constants.SEGMENTS_DIR).glob(f'{segments_dir_name}/*.wav')))
    if len(segment_audio_paths) == 0:
        typer.echo(f'Error: {segments_dir_name}/*.wav glob pattern matched no files.')
        typer.echo('=' * utils.GetTerminalColumnSize())
        sys.exit(1)
    for segment_audio_path in segment_audio_paths:
        segments_dir_name = segment_audio_path.parent.name
        typer.echo(f'Segment File: {segments_dir_name}/{segment_audio_path.name}')
    typer.echo('=' * utils.GetTerminalColumnSize())

    # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ›¸ãèµ·ã“ã—æ–‡ã‚’å–å¾—
    ## ä¾‹: 0001_ã“ã‚“ã«ã¡ã¯.wav -> ã“ã‚“ã«ã¡ã¯
    segment_audio_transcripts: list[str] = []
    for segment_audio_path in segment_audio_paths:

        # æ‹¡å¼µå­ãªã—ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ _ ã‚ˆã‚Šå¾Œã®éƒ¨åˆ†ã‚’å–å¾—
        segment_audio_transcript = segment_audio_path.stem.split('_')[1]

        # 1æ–‡ãŒé•·ã™ãã¦ãƒ•ã‚¡ã‚¤ãƒ«åãŒæœ€å¤§æ–‡å­—æ•°ã‚’è¶…ãˆã¦ã—ã¾ã£ã¦ã„ã‚‹å ´åˆã€åˆ¥é€”åŒã˜ãƒ•ã‚¡ã‚¤ãƒ«åã§ .txt ãƒ•ã‚¡ã‚¤ãƒ«ã«å…¨ä½“ã®æ›¸ãèµ·ã“ã—æ–‡ãŒä¿å­˜ã•ã‚Œã¦ã„ã‚‹ã®ã§ã€
        # ãã‚Œã‚’èª­ã¿è¾¼ã‚“ã§ä½¿ã†
        segment_audio_transcript_txt = segment_audio_path.with_suffix('.txt')
        if segment_audio_transcript_txt.exists():
            with open(segment_audio_transcript_txt, 'r', encoding='utf-8') as f:
                segment_audio_transcript = f.read()

        # æ›¸ãèµ·ã“ã—æ–‡ã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ 
        segment_audio_transcripts.append(segment_audio_transcript)

    # ç¾åœ¨å‡¦ç†ä¸­ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã¨æ›¸ãèµ·ã“ã—æ–‡
    current_index = 0

    # ã‚»ãƒ¬ã‚¯ãƒˆãƒœãƒƒã‚¯ã‚¹ã®é¸æŠè‚¢
    choices = ['ğŸš«ã“ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰é™¤å¤–ã™ã‚‹ğŸš«'] + speaker_name_list

    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®é€£ç•ª
    output_audio_count: dict[str, int] = {}
    for speaker in speaker_name_list:
        # æ—¢ã«ãã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«å­˜åœ¨ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸­ã§é€£ç•ªãŒä¸€ç•ªå¤§ãã„ã‚‚ã®ã‚’å–å¾—ã—ã€ãã‚Œã« 1 ã‚’è¶³ã—ãŸã‚‚ã®ã‚’åˆæœŸå€¤ã¨ã™ã‚‹
        output_audio_count[speaker] = max([
            int(re.sub(r'\D', '', i.stem)) for i in (constants.DATASETS_DIR / speaker / 'audios' / 'wavs').glob('*.wav')
        ], default=0) + 1

    def OnClick(
        segment_audio_path_str: str,
        speaker_name: str,
        transcript: str,
    ) -> tuple[gradio.Audio, gradio.Dropdown, gradio.Textbox]:
        """ ç¢ºå®šãƒœã‚¿ãƒ³ãŒæŠ¼ã•ã‚ŒãŸã¨ãã®å‡¦ç† """

        nonlocal current_index, segment_audio_paths, segment_audio_transcripts, choices, output_audio_count

        # è©±è€…åãŒç©ºã®å ´åˆã¯åˆæœŸç”»é¢ã‹ã‚‰ã€Œç¢ºå®šã€ã‚’æŠ¼ã—ã¦å®Ÿè¡Œã•ã‚ŒãŸã‚¤ãƒ™ãƒ³ãƒˆãªã®ã§ã€ä¿å­˜å‡¦ç†ã¯å®Ÿè¡Œã—ãªã„
        speaker_name = speaker_name.strip()
        if speaker_name != '' and speaker_name != 'é¸åˆ¥å®Œäº†':

            segment_audio_path = Path(segment_audio_path_str)
            typer.echo(f'Segment File : {segment_audio_path.name}')
            typer.echo(f'Speaker Name : {speaker_name}')
            typer.echo(f'Transcript   : {transcript}')

            # "ğŸš«ã“ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰é™¤å¤–ã™ã‚‹ğŸš«" ãŒé¸æŠã•ã‚ŒãŸå ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if speaker_name != 'ğŸš«ã“ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰é™¤å¤–ã™ã‚‹ğŸš«':

                # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ç·¨é›†å¾Œã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ (æ›¸ãèµ·ã“ã—æ–‡ã¯ãƒ•ã‚¡ã‚¤ãƒ«åãŒé•·ããªã‚‹ã®ã§å«ã¾ãšã€åˆ¥é€”ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹)
                ## Gradio ã®è¬æ©Ÿèƒ½ã§ã€GUI ã§ãƒˆãƒªãƒ ã—ãŸç·¨é›†å¾Œã®ä¸€æ¬¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒ segment_audio_path_str ã¨ã—ã¦æ¸¡ã•ã‚Œã¦ãã‚‹
                audio_output_dir = constants.DATASETS_DIR / speaker_name / 'audios' / 'wavs'
                audio_output_dir.mkdir(parents=True, exist_ok=True)
                output_path = audio_output_dir / f'{output_audio_count[speaker_name]:04}.wav'
                output_audio_count[speaker_name] += 1  # é€£ç•ªã‚’ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ãƒˆ
                shutil.copyfile(segment_audio_path, output_path)
                typer.echo(f'File {output_path} saved.')

                # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã¨æ›¸ãèµ·ã“ã—æ–‡ã®ãƒ‘ã‚¹ã®ãƒšã‚¢ã‚’ speaker.list ã«é †æ¬¡è¿½è¨˜
                text_list_path = constants.DATASETS_DIR / speaker_name / 'filelists' / 'speaker.list'
                if not text_list_path.exists():  # ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã‘ã‚Œã°ç©ºã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
                    text_list_path.parent.mkdir(parents=True, exist_ok=True)
                    text_list_path.touch()
                with open(text_list_path, 'a', encoding='utf-8') as f:
                    f.write(f'Data/{speaker_name}/audios/wavs/{output_path.name}|{speaker_name}|JP|{transcript}\n')
                typer.echo(f'File {text_list_path} updated.')
                typer.echo('-' * utils.GetTerminalColumnSize())

            else:
                typer.echo('Segment file skipped.')
                typer.echo('-' * utils.GetTerminalColumnSize())

            # æ¬¡ã®å‡¦ç†å¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            current_index += 1

        elif current_index < len(segment_audio_paths):
            # åˆæœŸç”»é¢ã‹ã‚‰ã€Œç¢ºå®šã€ã‚’æŠ¼ã—ã¦å®Ÿè¡Œã•ã‚ŒãŸã‚¤ãƒ™ãƒ³ãƒˆãªã®ã§ã€ãƒ­ã‚°ã«ç¢ºå®šã‚’å‡ºåŠ›
            ## æ¬¡ã®å‡¦ç†å¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„å ´åˆã¯å®Ÿè¡Œã•ã‚Œãªã„
            typer.echo('=' * utils.GetTerminalColumnSize())
            typer.echo('Selection of segment files has started.')
            typer.echo('=' * utils.GetTerminalColumnSize())

        # æ¬¡ã®å‡¦ç†å¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„å ´åˆã¯çµ‚äº†
        if current_index >= len(segment_audio_paths):
            typer.echo('=' * utils.GetTerminalColumnSize())
            typer.echo('All files processed.')
            typer.echo('=' * utils.GetTerminalColumnSize())
            return (
                gradio.Audio(
                    sources = [],
                    type = 'filepath',
                    interactive = True,
                    autoplay = True,
                ),
                gradio.Dropdown(choices=['é¸åˆ¥å®Œäº†'], value='é¸åˆ¥å®Œäº†', label='éŸ³å£°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®è©±è€…å'),  # type: ignore
                gradio.Textbox(value='ã™ã¹ã¦ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®é¸åˆ¥ã‚’å®Œäº†ã—ã¾ã—ãŸã€‚Aivis ã®ãƒ—ãƒ­ã‚»ã‚¹ã‚’çµ‚äº†ã—ã¦ãã ã•ã„ã€‚', label='éŸ³å£°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®æ›¸ãèµ·ã“ã—æ–‡'),
            )

        # UI ã‚’æ›´æ–°
        return (
            gradio.Audio(
                value = segment_audio_paths[current_index],
                sources = [],
                type = 'filepath',
                label = segment_audio_paths[current_index].name,
                interactive = True,
                autoplay = True,
            ),
            gradio.Dropdown(choices=choices, value=choices[0], label='éŸ³å£°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®è©±è€…å'),  # type: ignore
            gradio.Textbox(value=segment_audio_transcripts[current_index], label='éŸ³å£°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®æ›¸ãèµ·ã“ã—æ–‡'),
        )

    def OnReset(speaker_name: str) -> tuple[gradio.Audio, gradio.Textbox]:
        """ ãƒªã‚»ãƒƒãƒˆãƒœã‚¿ãƒ³ãŒæŠ¼ã•ã‚ŒãŸã¨ãã®å‡¦ç† """

        nonlocal current_index, segment_audio_paths, segment_audio_transcripts, choices

        # è©±è€…åãŒç©ºã®å ´åˆã¯åˆæœŸç”»é¢ã‹ã‚‰ã€Œç¢ºå®šã€ã‚’æŠ¼ã—ã¦å®Ÿè¡Œã•ã‚ŒãŸã‚¤ãƒ™ãƒ³ãƒˆãªã®ã§ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ•ã‚©ãƒ¼ãƒ ã‚’è¿”ã™
        if speaker_name == '':
            return (
                gradio.Audio(
                    sources = [],
                    type = 'filepath',
                    interactive = True,
                    autoplay = True,
                ),
                gradio.Textbox(value='ç¢ºå®šãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚', label='éŸ³å£°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®æ›¸ãèµ·ã“ã—æ–‡'),
            )

        # ç¾åœ¨ã® current_index ã«å¿œã˜ã¦éŸ³å£°ã¨æ›¸ãèµ·ã“ã—æ–‡ã‚’ãƒªã‚»ãƒƒãƒˆ
        return (
            gradio.Audio(
                value = segment_audio_paths[current_index],
                sources = [],
                type = 'filepath',
                label = segment_audio_paths[current_index].name,
                interactive = True,
                autoplay = True,
            ),
            gradio.Textbox(value=segment_audio_transcripts[current_index], label='éŸ³å£°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®æ›¸ãèµ·ã“ã—æ–‡'),
        )

    # Gradio UI ã®å®šç¾©ã¨èµ·å‹•
    with gradio.Blocks(css='.gradio-container { max-width: 768px !important; }') as gui:
        with gradio.Column():
            gradio.Markdown("""
                # Aivis - Create Datasets
            """)
            audio_player = gradio.Audio(
                sources = [],
                type = 'filepath',
                interactive = True,
                autoplay = True,
            )
            speaker_choice = gradio.Dropdown(choices=[], value='', label='éŸ³å£°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®è©±è€…å')  # type: ignore
            transcript_box = gradio.Textbox(value='ç¢ºå®šãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚', label='éŸ³å£°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®æ›¸ãèµ·ã“ã—æ–‡')
            confirm_button = gradio.Button('ç¢ºå®š')
            confirm_button.click(
                fn = OnClick,
                inputs = [
                    audio_player,
                    speaker_choice,
                    transcript_box,
                ],
                outputs = [
                    audio_player,
                    speaker_choice,
                    transcript_box,
                ],
            )
            reset_button = gradio.Button('éŸ³å£°ã¨æ›¸ãèµ·ã“ã—æ–‡ã®å¤‰æ›´ã‚’ãƒªã‚»ãƒƒãƒˆ')
            reset_button.click(
                fn = OnReset,
                inputs = [
                    speaker_choice,
                ],
                outputs = [
                    audio_player,
                    transcript_box,
                ],
            )

        # 0.0.0.0:7860 ã§ Gradio UI ã‚’èµ·å‹•
        gui.launch(server_name='0.0.0.0', server_port=7860)


@app.command(help='Check dataset files and calculate total duration.')
def check_dataset(
    speaker_name: Annotated[str, typer.Argument(help='Speaker name.')],
):
    typer.echo('=' * utils.GetTerminalColumnSize())

    # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    dataset_dir = constants.DATASETS_DIR / speaker_name
    if not dataset_dir.exists():
        typer.echo(f'Error: Speaker {speaker_name} not found.')
        typer.echo('=' * utils.GetTerminalColumnSize())
        sys.exit(1)

    # speaker.list ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã¨æ›¸ãèµ·ã“ã—æ–‡ã‚’å–å¾—
    ## ä¾‹: Data/SpeakerName/audios/wavs/0001_ã“ã‚“ã«ã¡ã¯.wav|SpeakerName|JP|ã“ã‚“ã«ã¡ã¯
    with open(dataset_dir / 'filelists' / 'speaker.list', 'r', encoding='utf-8') as f:
        dataset_files_raw = f.read().splitlines()
        dataset_files = [i.split('|') for i in dataset_files_raw]

    typer.echo(f'Speaker: {speaker_name} / Directory: {dataset_dir}')
    typer.echo('=' * utils.GetTerminalColumnSize())

    # å„éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«ãƒ«ãƒ¼ãƒ—
    total_audio_duration = 0.0
    for index, dataset_file in enumerate(dataset_files):
        if index > 0:
            typer.echo('-' * utils.GetTerminalColumnSize())
        dataset_file_path = Path(dataset_file[0].replace('Data/', constants.DATASETS_DIR.as_posix() + '/'))
        typer.echo(f'Dataset File : {dataset_file_path}')
        if not dataset_file_path.exists():
            typer.echo(f'Error: Dataset file {dataset_file_path} not found.')
        else:
            audio_duration = prepare.GetAudioFileDuration(dataset_file_path)
            total_audio_duration += audio_duration
            typer.echo(f'Duration     : {utils.SecondToTimeCode(audio_duration)}')
            typer.echo(f'Transcript   : {dataset_file[3]}')

    typer.echo('=' * utils.GetTerminalColumnSize())
    typer.echo(f'Total Files    : {len(dataset_files)}')
    typer.echo(f'Total Duration : {utils.SecondToTimeCode(total_audio_duration)}')
    typer.echo('=' * utils.GetTerminalColumnSize())


@app.command(help='Train model.')
def train(
    speaker_name: Annotated[str, typer.Argument(help='Speaker name.')],
    epochs: Annotated[int, typer.Option(help='Training epochs.')] = 50,
    batch_size: Annotated[int, typer.Option(help='Training batch size.')] = 4,
):
    typer.echo('=' * utils.GetTerminalColumnSize())

    # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    dataset_dir = constants.DATASETS_DIR / speaker_name
    if not dataset_dir.exists():
        typer.echo(f'Error: Speaker {speaker_name} not found.')
        typer.echo('=' * utils.GetTerminalColumnSize())
        sys.exit(1)

    typer.echo(f'Speaker: {speaker_name} / Directory: {dataset_dir}')
    typer.echo(f'Epochs: {epochs} / Batch Size: {batch_size}')
    typer.echo('=' * utils.GetTerminalColumnSize())

    # Bert-VITS2 ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    bert_vits2_dataset_dir = constants.BERT_VITS2_DIR / 'Data'
    bert_vits2_dataset_dir.mkdir(parents=True, exist_ok=True)

    # äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒã¾ã ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãªã‘ã‚Œã°ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    ## ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­ã«å®Ÿè¡Œã‚’ä¸­æ–­ã™ã‚‹ã¨ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é€”ä¸­ã®ãƒ­ãƒ¼ãƒ‰ã§ããªã„äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒæ®‹ã£ã¦ã—ã¾ã†
    ## åŸºæœ¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­ã«å®Ÿè¡Œã‚’ä¸­æ–­ã™ã¹ãã§ã¯ãªã„ãŒã€ä¸‡ãŒä¸€ãã†ãªã£ãŸå ´åˆã¯æ‰‹å‹•ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é€”ä¸­ã®ãƒ¢ãƒ‡ãƒ«ã‚’å‰Šé™¤ã—ã¦ã‹ã‚‰å†å®Ÿè¡Œã™ã‚‹å¿…è¦ãŒã‚ã‚‹
    download_base_url = 'https://huggingface.co/OedoSoldier/Bert-VITS2-2.3/resolve/main/'
    if not (bert_vits2_dataset_dir / 'DUR_0.pth').exists():
        typer.echo('Downloading pretrained model (DUR_0.pth) ...')
        utils.DownloadFile(download_base_url + 'DUR_0.pth', bert_vits2_dataset_dir / 'DUR_0.pth')
    if not (bert_vits2_dataset_dir / 'D_0.pth').exists():
        typer.echo('Downloading pretrained model (D_0.pth) ...')
        utils.DownloadFile(download_base_url + 'D_0.pth', bert_vits2_dataset_dir / 'D_0.pth')
    if not (bert_vits2_dataset_dir / 'G_0.pth').exists():
        typer.echo('Downloading pretrained model (G_0.pth) ...')
        utils.DownloadFile(download_base_url + 'G_0.pth', bert_vits2_dataset_dir / 'G_0.pth')
    if not (bert_vits2_dataset_dir / 'WD_0.pth').exists():
        typer.echo('Downloading pretrained model (WD_0.pth) ...')
        utils.DownloadFile(download_base_url + 'WD_0.pth', bert_vits2_dataset_dir / 'WD_0.pth')

    # æ—¢ã« Bert-VITS2/Data/(è©±è€…å)/audios/ ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ä¸€æ—¦å‰Šé™¤
    ## åŒä¸€ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚‚ã†ä¸€åº¦å­¦ç¿’ã‚’å›ã™éš›ã€Bert é–¢é€£ã®ä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¦å†ç”Ÿæˆã•ã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹
    if (bert_vits2_dataset_dir / speaker_name / 'audios').exists():
        shutil.rmtree(bert_vits2_dataset_dir / speaker_name / 'audios')

    # æ—¢ã« Bert-VITS2/Data/(è©±è€…å)/filelists/ ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ä¸€æ—¦å‰Šé™¤
    ## åŒä¸€ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚‚ã†ä¸€åº¦å­¦ç¿’ã‚’å›ã™éš›ã€æ›¸ãèµ·ã“ã—ãƒ‡ãƒ¼ã‚¿ã®ä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¦å†ç”Ÿæˆã•ã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹
    if (bert_vits2_dataset_dir / speaker_name / 'filelists').exists():
        shutil.rmtree(bert_vits2_dataset_dir / speaker_name / 'filelists')

    # æŒ‡å®šã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ Bert-VITS2 ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚³ãƒ”ãƒ¼
    ## ex: 04-Datasets/(è©±è€…å)/audios/ -> Bert-VITS2/Data/(è©±è€…å)/audios/
    ## ex: 04-Datasets/(è©±è€…å)/filelists/ -> Bert-VITS2/Data/(è©±è€…å)/filelists/
    typer.echo('Copying dataset files...')
    shutil.copytree(dataset_dir / 'audios', bert_vits2_dataset_dir / speaker_name / 'audios')
    shutil.copytree(dataset_dir / 'filelists', bert_vits2_dataset_dir / speaker_name / 'filelists')

    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸäº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ Bert-VITS2/Data/(è©±è€…å)/models/ ã«ã‚³ãƒ”ãƒ¼
    ## ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã®éš›ã«ã“ã‚Œã‚‰ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ä¸Šæ›¸ãã•ã‚Œã¦ã—ã¾ã†ãŸã‚ã€ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ã§ã¯ãªãã‚³ãƒ”ãƒ¼ã™ã‚‹
    if not (bert_vits2_dataset_dir / speaker_name / 'models').exists():
        typer.echo('Copying pretrained model files...')
        (bert_vits2_dataset_dir / speaker_name / 'models').mkdir(parents=True, exist_ok=True)
        ## ex: Bert-VITS2/Data/DUR_0.pth -> Bert-VITS2/Data/(è©±è€…å)/models/DUR_0.pth
        if not (bert_vits2_dataset_dir / speaker_name / 'models' / 'DUR_0.pth').exists():
            shutil.copyfile(bert_vits2_dataset_dir / 'DUR_0.pth', bert_vits2_dataset_dir / speaker_name / 'models' / 'DUR_0.pth')
        if not (bert_vits2_dataset_dir / speaker_name / 'models' / 'D_0.pth').exists():
            shutil.copyfile(bert_vits2_dataset_dir / 'D_0.pth', bert_vits2_dataset_dir / speaker_name / 'models' / 'D_0.pth')
        if not (bert_vits2_dataset_dir / speaker_name / 'models' / 'G_0.pth').exists():
            shutil.copyfile(bert_vits2_dataset_dir / 'G_0.pth', bert_vits2_dataset_dir / speaker_name / 'models' / 'G_0.pth')
        if not (bert_vits2_dataset_dir / speaker_name / 'models' / 'WD_0.pth').exists():
            shutil.copyfile(bert_vits2_dataset_dir / 'WD_0.pth', bert_vits2_dataset_dir / speaker_name / 'models' / 'WD_0.pth')

    # Bert-VITS2/configs/config.json ã‚’ Bert-VITS2/Data/(è©±è€…å)/config.json ã«ã‚³ãƒ”ãƒ¼
    ## ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã®éš›ã«ã“ã‚Œã‚‰ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ä¸Šæ›¸ãã•ã‚Œã¦ã—ã¾ã†ãŸã‚ã€ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ã§ã¯ãªãã‚³ãƒ”ãƒ¼ã™ã‚‹
    if not (bert_vits2_dataset_dir / speaker_name / 'config.json').exists():
        typer.echo('Copying model config file...')
        shutil.copyfile(constants.BERT_VITS2_DIR / 'configs' / 'config.json', bert_vits2_dataset_dir / speaker_name / 'config.json')

    # ã‚³ãƒ”ãƒ¼ã—ãŸ config.json ã® epochs ã¨ batch_size ã¨ã‚’æŒ‡å®šã•ã‚ŒãŸå€¤ã«å¤‰æ›´
    with open(bert_vits2_dataset_dir / speaker_name / 'config.json', 'r', encoding='utf-8') as f:
        config = json.load(f)
    config['train']['epochs'] = epochs
    config['train']['batch_size'] = batch_size
    with open(bert_vits2_dataset_dir / speaker_name / 'config.json', 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)

    # Bert-VITS2/default_config.yml ã‚’ Bert-VITS2/config.yml ã«ã‚³ãƒ”ãƒ¼
    ## å­¦ç¿’å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã‚’å¤‰æ›´ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ãŸã‚ã€æ—¢ã« config.yml ãŒå­˜åœ¨ã™ã‚‹å ´åˆã‚‚ä¸Šæ›¸ãã™ã‚‹
    typer.echo('Copying default_config.yml to config.yml...')
    shutil.copyfile(constants.BERT_VITS2_DIR / 'default_config.yml', constants.BERT_VITS2_DIR / 'config.yml')

    # config.yml å†…ã® dataset_path: "Data/MySpeaker" ã‚’ dataset_path: "Data/(è©±è€…å)" ã«å¤‰æ›´
    ## æ­£è¦è¡¨ç¾ã§ç½®æ›ã™ã‚‹
    with open(constants.BERT_VITS2_DIR / 'config.yml', 'r', encoding='utf-8') as f:
        config_yml = f.read()
    config_yml = re.sub(r'dataset_path: "Data/.*"', f'dataset_path: "Data/{speaker_name}"', config_yml)
    with open(constants.BERT_VITS2_DIR / 'config.yml', 'w', encoding='utf-8') as f:
        f.write(config_yml)
    typer.echo('=' * utils.GetTerminalColumnSize())

    # Bert-VITS2/preprocess_text.py ã‚’å®Ÿè¡Œ
    typer.echo('Running preprocess_text.py...')
    typer.echo('-' * utils.GetTerminalColumnSize())
    subprocess.run(
        ['python', constants.BERT_VITS2_DIR / 'preprocess_text.py'],
        cwd = constants.BERT_VITS2_DIR,  # ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ Bert-VITS2/ ã«å¤‰æ›´ã—ãªã„ã¨å®Ÿè¡Œã§ããªã„
        check = True,
    )
    typer.echo('=' * utils.GetTerminalColumnSize())

    # Bert-VITS2/bert_gen.py ã‚’å®Ÿè¡Œ
    typer.echo('Running bert_gen.py...')
    typer.echo('-' * utils.GetTerminalColumnSize())
    subprocess.run(
        ['python', constants.BERT_VITS2_DIR / 'bert_gen.py'],
        cwd = constants.BERT_VITS2_DIR,  # ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ Bert-VITS2/ ã«å¤‰æ›´ã—ãªã„ã¨å®Ÿè¡Œã§ããªã„
        check = True,
    )
    typer.echo('=' * utils.GetTerminalColumnSize())

    # å­¦ç¿’ã‚’é–‹å§‹ (Bert-VITS2/train_ms.py ã‚’å®Ÿè¡Œ)
    typer.echo('Training started.')
    typer.echo('-' * utils.GetTerminalColumnSize())
    try:
        subprocess.run(
            ['python', constants.BERT_VITS2_DIR / 'train_ms.py'],
            cwd = constants.BERT_VITS2_DIR,  # ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ Bert-VITS2/ ã«å¤‰æ›´ã—ãªã„ã¨å®Ÿè¡Œã§ããªã„
            check = True,
        )
    except subprocess.CalledProcessError as ex:
        typer.echo('-' * utils.GetTerminalColumnSize())
        typer.echo(f'Training failed. (Process exited with code {ex.returncode})')
        typer.echo('=' * utils.GetTerminalColumnSize())
        sys.exit(1)
    typer.echo('-' * utils.GetTerminalColumnSize())
    typer.echo('Training finished.')
    typer.echo('=' * utils.GetTerminalColumnSize())


@app.command(help='Infer model.')
def infer(
    speaker_name: Annotated[str, typer.Argument(help='Speaker name.')],
    model_step: Annotated[Optional[int], typer.Option(help='Model step. (Default: Largest step)')] = None,
):
    typer.echo('=' * utils.GetTerminalColumnSize())

    # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    model_dir = constants.BERT_VITS2_DIR / 'Data' / speaker_name
    if not model_dir.exists():
        typer.echo(f'Error: Speaker {speaker_name} not found.')
        typer.echo('=' * utils.GetTerminalColumnSize())
        sys.exit(1)

    # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
    # æŒ‡å®šã•ã‚Œã¦ã„ãªã‘ã‚Œã°æœ€å¤§ã®ã‚¹ãƒ†ãƒƒãƒ—ã®ãƒ¢ãƒ‡ãƒ«ã‚’æ¢ã™
    ## ãƒ¢ãƒ‡ãƒ«ã¯ 1000 ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ä¿å­˜ã•ã‚Œã¦ãŠã‚Šã€G_(ã‚¹ãƒ†ãƒƒãƒ—æ•°).pth ã®ãƒ•ã‚¡ã‚¤ãƒ«åãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ä¿å­˜ã•ã‚Œã¦ã„ã‚‹
    ## ä¾‹: G_0.pth / G_1000.pth / G_2000.pth / G_3000.pth
    if model_step is None:
        model_step = 0
        for model_file in (model_dir / 'models').glob('G_*.pth'):
            step = int(re.sub(r'\D', '', model_file.stem))
            if step > model_step:
                model_step = step
        if (model_dir / 'models' / f'G_{model_step}.pth').exists():
            model_file = model_dir / 'models' / f'G_{model_step}.pth'
        else:
            typer.echo(f'Error: Model file {model_dir / "models" / f"G_{model_step}.pth"} not found.')
            typer.echo('=' * utils.GetTerminalColumnSize())
            sys.exit(1)

    # ã‚¹ãƒ†ãƒƒãƒ—æ•°ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãã®ã‚¹ãƒ†ãƒƒãƒ—ã®ãƒ¢ãƒ‡ãƒ«ã‚’æ¢ã™
    else:
        model_file = model_dir / 'models' / f'G_{model_step}.pth'
        if not model_file.exists():
            typer.echo(f'Error: Model file {model_file} not found.')
            typer.echo('=' * utils.GetTerminalColumnSize())
            sys.exit(1)

    typer.echo(f'Speaker: {speaker_name} / Model Directory: {model_dir}')
    typer.echo(f'Model File: {model_file}')
    typer.echo('=' * utils.GetTerminalColumnSize())

    # config.yml ã‚’æ­£è¦è¡¨ç¾ã§æ›¸ãæ›ãˆã‚‹
    ## dataset_path: ".*" ã‚’ dataset_path: "Data/(è©±è€…å)" ã«æ›¸ãæ›ãˆã‚‹
    ## model: "models/.*" ã‚’ model: "models/G_(ã‚¹ãƒ†ãƒƒãƒ—æ•°).pth" ã«æ›¸ãæ›ãˆã‚‹
    with open(constants.BERT_VITS2_DIR / 'config.yml', 'r', encoding='utf-8') as f:
        config_yml = f.read()
    config_yml = re.sub(r'dataset_path: ".*"', f'dataset_path: "Data/{speaker_name}"', config_yml)
    config_yml = re.sub(r'model: "models/.*"', f'model: "models/G_{model_step}.pth"', config_yml)
    with open(constants.BERT_VITS2_DIR / 'config.yml', 'w', encoding='utf-8') as f:
        f.write(config_yml)

    # Bert-VITS2/webui.py ã‚’å®Ÿè¡Œ
    typer.echo('Running Infer Web UI...')
    typer.echo('-' * utils.GetTerminalColumnSize())
    subprocess.run(
        ['python', constants.BERT_VITS2_DIR / 'webui.py'],
        cwd = constants.BERT_VITS2_DIR,  # ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ Bert-VITS2/ ã«å¤‰æ›´ã—ãªã„ã¨å®Ÿè¡Œã§ããªã„
        check = True,
    )
    typer.echo('=' * utils.GetTerminalColumnSize())


@app.command(help='Show version.')
def version():
    typer.echo(f'Aivis version {__version__}')


if __name__ == '__main__':
    app()
